{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##*************************Task 1*********************#\n",
    "#\n",
    "#   (Class \"Critic\" should be a subclass of the class CriticBase. You must use the exact class name.)\n",
    "#   You should implement a multi-layer (2 or 3 layers) LSTM model in this class.\n",
    "#   The Model (the score function) takes a sequence of envents as input and outputs a score judging\n",
    "#   whether the piano music corresponding to the sequence is good music or bad music.\n",
    "#   A function to generate random music is provided in the \"midi2seq.py\".\n",
    "#   Use the function to create a collection of random piano plays as examples of bad music.\n",
    "#   Use the piano plays in the downloaded data as example of good music.\n",
    "#   (You don't need to use all the downloaded data. A sufficiently large subset will be enough.)\n",
    "#   Train the model in this class using both the good and the bad examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midi2seq import process_midi_seq, random_piano\n",
    "import glob\n",
    "good_music_midi = process_midi_seq(datadir='.', n=5000, maxlen=100, rd_seed=1000)\n",
    "\n",
    "bad_music_midi = [random_piano(n=5000) for _ in range(good_music_midi.shape[1])]\n",
    "bad_music_midi = process_midi_seq(all_midis=bad_music_midi, datadir='.', n=5000, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5706, 101), (5567, 101))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_music_midi.shape, bad_music_midi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('good_music_midi.npy', 'wb') as f:\n",
    "    np.save(f, good_music_midi)\n",
    "\n",
    "with open('bad_music_midi.npy', 'wb') as f:\n",
    "    np.save(f, bad_music_midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5706, 101])\n",
      "torch.Size([5706, 101, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "num_embeddings = 382\n",
    "embedding_dim = 100\n",
    "embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "good_music_midi = torch.tensor(good_music_midi, dtype=torch.long)\n",
    "print(good_music_midi.shape)\n",
    "\n",
    "embedded = embedding(good_music_midi)\n",
    "print(embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5706, 101, 128])\n",
      "torch.Size([5706, 2])\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 64\n",
    "n_labels = 2\n",
    "\n",
    "rnn = nn.LSTM(input_size=embedding_dim, hidden_size=rnn_size, bidirectional=True, num_layers=3)\n",
    "top_layer = nn.Linear(2*rnn_size, n_labels)\n",
    "\n",
    "rnn_out, _ = rnn(embedded)\n",
    "print(rnn_out.shape)\n",
    "\n",
    "out = top_layer(rnn_out[:, -1, :])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0273,  0.0025],\n",
       "        [ 0.0378,  0.0040],\n",
       "        [ 0.0421,  0.0045],\n",
       "        ...,\n",
       "        [ 0.0430, -0.0043],\n",
       "        [ 0.0462, -0.0053],\n",
       "        [ 0.0474,  0.0016]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 14.68 GB, other allocations: 6.58 GB, max allowed: 18.13 GB). Tried to allocate 44.75 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb#X33sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model \u001b[39m=\u001b[39m LSTMCritic()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb#X33sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m pred \u001b[39m=\u001b[39m model(good_music_midi\u001b[39m.\u001b[39;49mto(model\u001b[39m.\u001b[39;49mdevice))\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb#X33sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m out \u001b[39m=\u001b[39m lstm_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Linear layer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic_R2.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 14.68 GB, other allocations: 6.58 GB, max allowed: 18.13 GB). Tried to allocate 44.75 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# Critic model\n",
    "# n x max_len -> embedding -> n x nax_len x max_len - 1 -> LSTM (with hidden size=3)-> n x 2\n",
    "class LSTMCritic(nn.Module):\n",
    "    def __init__(self, num_embeddings=382, embedding_dim=100, hidden_dim=128, num_layers=3, n_classes=2):\n",
    "        super(LSTMCritic, self).__init__()\n",
    "        self.num_embeddings = num_embeddings # number of unique words in the vocabulary\n",
    "        self.embedding_dim = embedding_dim #\n",
    "        self.hidden_dim = hidden_dim # Hidden dimension\n",
    "        self.num_layers = num_layers # Number of LSTM layers\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim).to(self.device)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=0.2).to(self.device)\n",
    "        self.fc = nn.Linear(self.hidden_dim, n_classes).to(self.device)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x).to(self.device)\n",
    "        # LSTM forward pass\n",
    "        batch_size = x.size(0)\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = lstm_out[:, -1, :]\n",
    "        # Linear layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = LSTMCritic()\n",
    "pred = model(good_music_midi.to(model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post - Tuesday Oct 10, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import os, sys, time, datetime, pickle, copy, random, glob, logging\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from midi2seq import piano2seq, random_piano, process_midi_seq\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model_base import ComposerBase, CriticBase\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data\n",
    "class MidiDataProcessor:\n",
    "\n",
    "    def __init__(self, data_directory, maxlen=100, test_size=0.2, random_state=42, batch_size=32):\n",
    "        self.data_directory = data_directory\n",
    "        self.maxlen = maxlen\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __get__(self, idx):\n",
    "        return self.all_data[idx], self.all_labels[idx]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "        all_midis = glob.glob(f'{self.data_directory}/maestro-v1.0.0/**/*.midi')\n",
    "\n",
    "        good_music_midi = process_midi_seq(all_midis=all_midis, datadir=self.data_directory, n=10000, maxlen=self.maxlen)\n",
    "        bad_music_midi = [random_piano(n=self.maxlen) for _ in range(len(all_midis))]\n",
    "        bad_music_midi = process_midi_seq(all_midis=bad_music_midi, datadir=self.data_directory, n=10000, maxlen=self.maxlen)\n",
    "\n",
    "        good_music = torch.tensor(good_music_midi, dtype=torch.float32)\n",
    "        bad_music = torch.tensor(bad_music_midi, dtype=torch.float32)\n",
    "\n",
    "        good_labels = torch.ones((len(good_music), 1))\n",
    "        bad_labels = torch.zeros((len(bad_music), 1))\n",
    "\n",
    "        self.all_data = torch.cat([good_music, bad_music], dim=0)\n",
    "        self.all_labels = torch.cat([good_labels, bad_labels], dim=0)\n",
    "\n",
    "        features_train, features_test, label_train, label_test = train_test_split(\n",
    "                                                                                    self.all_data, self.all_labels,\n",
    "                                                                                    test_size=self.test_size,\n",
    "                                                                                    random_state=self.random_state,\n",
    "                                                                                    shuffle=True)\n",
    "        \n",
    "\n",
    "        def convert_labels(labels):\n",
    "            converted = torch.zeros(labels.size(0), 2)\n",
    "            converted[labels.view(-1) == 1, 0] = 1\n",
    "            converted[labels.view(-1) == 0, 1] = 1\n",
    "            return converted\n",
    "        \n",
    "        # label_train = convert_labels(label_train)\n",
    "        # label_test = convert_labels(label_test)\n",
    "\n",
    "        features_train = torch.Tensor(features_train).to(device)\n",
    "        features_test = torch.Tensor(features_test).to(device)\n",
    "\n",
    "        label_train = torch.Tensor(label_train).to(device)\n",
    "        label_test = torch.Tensor(label_test).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(features_train, label_train)\n",
    "        test_dataset = TensorDataset(features_test, label_test)\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "        return self.train_loader, self.test_loader\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MidiDataProcessor(data_directory={self.data_directory!r}, maxlen={self.maxlen}, test_size={self.test_size}, random_state={self.random_state}, batch_size={self.batch_size}, train_loader size={len(self.train_loader.dataset)}, test_loader size={len(self.test_loader.dataset)})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MidiDataProcessor(data_directory='.')\n",
    "train_loader, test_loader = processor.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MidiDataProcessor(data_directory='.', maxlen=100, test_size=0.2, random_state=42, batch_size=32, train_loader size=16140, test_loader size=4035)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([256., 190., 285., 256., 366.,  38., 306., 256., 370., 118., 264., 246.,\n",
       "         285., 256., 364.,   8., 257., 220., 266., 136., 282., 256., 371., 111.,\n",
       "         280., 256., 217., 259., 256., 227., 258., 256., 166., 264., 364.,  67.,\n",
       "         269., 256., 195., 281., 256., 369.,  60., 298., 256., 366., 100., 267.,\n",
       "         367.,  26., 239., 264., 370.,  46., 312., 256., 367.,  56., 269., 256.,\n",
       "         184., 268., 256., 154., 271., 256., 367.,  31., 281., 256., 174., 259.,\n",
       "         256., 188., 281., 256., 159., 263., 361.,  54., 272., 256., 362.,  59.,\n",
       "         280., 256., 182., 260., 256., 228., 257., 370.,  63., 262., 191., 266.,\n",
       "         363.,  53., 257., 365.,   6.], device='mps:0'),\n",
       " tensor([0.], device='mps:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(labels):\n",
    "    converted = torch.zeros(labels.size(0), 2)\n",
    "    converted[labels.view(-1) == 1, 0] = 1\n",
    "    converted[labels.view(-1) == 0, 1] = 1\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 101]) torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, label) in enumerate(train_loader):\n",
    "    print(data.shape, convert_labels(label).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], device='mps:0'))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_labels(label[:10]), label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic model\n",
    "# n x max_len -> embedding -> n x nax_len x max_len - 1 -> LSTM (with hidden size=3)-> n x 2\n",
    "# n x max_len -> embedding -> n x nax_len x max_len - 1 -> LSTM (with hidden size=3)-> n x 2\n",
    "class LSTMCritic(nn.Module):\n",
    "    def __init__(self, num_embeddings=382, embedding_dim=100, hidden_dim=128, num_layers=3, n_classes=2):\n",
    "        super(LSTMCritic, self).__init__()\n",
    "        self.num_embeddings = num_embeddings # number of unique words in the vocabulary\n",
    "        self.embedding_dim = embedding_dim #\n",
    "        self.hidden_dim = hidden_dim # Hidden dimension\n",
    "        self.num_layers = num_layers # Number of LSTM layers\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim).to(self.device)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=0.2).to(self.device)\n",
    "        self.fc = nn.Linear(self.hidden_dim, n_classes).to(self.device)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x).to(self.device)\n",
    "        # LSTM forward pass\n",
    "        batch_size = x.size(0)\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = lstm_out[:, -1, :]\n",
    "        # Linear layer\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccumulationMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.value = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0\n",
    "        self.count = 0.0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.value = value\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.sqrt = self.value ** 0.5\n",
    "        self.rmse = self.avg ** 0.5\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic class\n",
    "class Critic(CriticBase):\n",
    "    def __init__(self, load_trained=False):\n",
    "        '''\n",
    "        :param load_trained\n",
    "            If load_trained is True, load a trained model from a file.\n",
    "            Should include code to download the file from Google drive if necessary.\n",
    "            else, construct the model\n",
    "        '''    \n",
    "    \n",
    "        self.load_trained = load_trained\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "        self.model = LSTMCritic(vocab_size=500, hidden_dim=128, num_layers=3, n_classes=2).to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        if self.load_trained:\n",
    "            logging.info('load model from file ...')\n",
    "            gdd.download_file_from_google_drive(file_id='18YkTrsqa0dWCVC4PpE2_7q8nN3jxdzhD',\n",
    "                                    dest_path='./critic.pth',\n",
    "                                    unzip=True)\n",
    "            self.model = torch.load('critic.pth')\n",
    "            self.model.eval()\n",
    "\n",
    "    def score(self, x):\n",
    "        '''\n",
    "        Compute the score of a music sequence\n",
    "        :param x: a music sequence\n",
    "        :return: the score between 0 and 1 that reflects the quality of the music: the closer to 1, the better\n",
    "        '''\n",
    "        with torch.set_grad_enabled(False):\n",
    "            logging.info('Compute score ...')\n",
    "        \n",
    "            outputs = self.model(x.to(self.device))\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            outputs ^= 1 # index 0 is good and index 1 is bad \n",
    "                    \n",
    "        return outputs  \n",
    "    \n",
    "    def validate(self, val_loader, model):\n",
    "        \"\"\"Evaluate the network on the entire validation set.\"\"\"\n",
    "\n",
    "        loss_accum = AccumulationMeter()\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            for i, (feature, label) in enumerate(val_loader):\n",
    "                feature, label = feature.to(self.device).long(), label.to(self.device)\n",
    "\n",
    "                outputs = self.model(feature)\n",
    "\n",
    "                loss = self.criterion(outputs, label)\n",
    "                loss_accum.update(loss.item(), label.size(0))\n",
    " \n",
    "\n",
    "        return loss_accum.rmse\n",
    "\n",
    "    def train(self, x, epochs=10, lr=1e-5):\n",
    "        '''\n",
    "        Train the model on one batch of data\n",
    "        :param x: train data. For critic training, x will be a tuple of two tensors (data, label). expect a batch of dataloader\n",
    "        :return: (mean) loss of the model on the batch\n",
    "        '''\n",
    "            \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        loss_accum_train = AccumulationMeter()\n",
    "\n",
    "        # split data for K-fold cross validation to avoid overfitting\n",
    "        indices = list(range(len(x.dataset)))\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        cv_index = 0\n",
    "        index_list_train = []\n",
    "        index_list_valid = []\n",
    "        for train_indices, valid_indices in kf.split(indices):\n",
    "            index_list_train.append(train_indices)\n",
    "            index_list_valid.append(valid_indices)\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "            train_loader = DataLoader(x.dataset, batch_size=32,\n",
    "                                                       sampler=train_sampler,\n",
    "                                                       shuffle=False)\n",
    "            val_loader = DataLoader(x.dataset, batch_size=32,\n",
    "                                                     sampler=valid_sampler,\n",
    "                                                     shuffle=False)\n",
    "\n",
    "            logging.info('Start training ...')\n",
    "            self.model.train()\n",
    "            early_stopping = EarlyStopping(tolerance=5, min_delta=10)\n",
    "            epoch_train_loss = []\n",
    "            epoch_validate_loss = []\n",
    "            for epoch in range(epochs):\n",
    "                for idx, (feature, label) in enumerate(train_loader):\n",
    "                    feature, label = feature.to(self.device).long(), label.to(self.device)\n",
    "\n",
    "                    outputs = self.model(feature)\n",
    "                    loss = self.criterion(outputs, label)\n",
    "                    #total_loss += loss.item()\n",
    "                    loss_accum_train.update(loss.item(), label.size(0))\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                # training loss\n",
    "                epoch_train_loss.append(loss_accum_train.avg)\n",
    "                \n",
    "                # validation loss\n",
    "                val_loss_avg = self.validate(val_loader, self.model)\n",
    "                epoch_validate_loss.append(val_loss_avg)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss_accum_train.avg}, Val Loss: {val_loss_avg}\")\n",
    "\n",
    "                # early stopping\n",
    "                early_stopping(loss_accum_train.avg, val_loss_avg)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"We are at epoch:\", epoch)\n",
    "                    logging.info(\"Finished training ...Model saved\")\n",
    "                    torch.save(self.model, 'critic.pth') \n",
    "                    break\n",
    "        \n",
    "            cv_index += 1   # increment cv index\n",
    "        \n",
    "        return loss_accum_train.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Start training ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6917002614877992, Val Loss: 0.8294968625707562\n",
      "Epoch 2/100, Train Loss: 0.6691033990720859, Val Loss: 0.6616604525014153\n",
      "Epoch 3/100, Train Loss: 0.4930773457685144, Val Loss: 0.26094386346398285\n",
      "Epoch 4/100, Train Loss: 0.3847082929710066, Val Loss: 0.23794809452955532\n",
      "Epoch 5/100, Train Loss: 0.31714117089455124, Val Loss: 0.22171827681297912\n",
      "Epoch 6/100, Train Loss: 0.2714204840186782, Val Loss: 0.21149176010041\n",
      "Epoch 7/100, Train Loss: 0.23789563396844443, Val Loss: 0.20193328822853523\n",
      "Epoch 8/100, Train Loss: 0.212300402979822, Val Loss: 0.19181441002380759\n",
      "Epoch 9/100, Train Loss: 0.19182193755770774, Val Loss: 0.18392870302532874\n",
      "Epoch 10/100, Train Loss: 0.17528449735721488, Val Loss: 0.1728595726870176\n",
      "Epoch 11/100, Train Loss: 0.16144992917491977, Val Loss: 0.16551293523815055\n",
      "Epoch 12/100, Train Loss: 0.14978490334799432, Val Loss: 0.17387458425859195\n",
      "Epoch 13/100, Train Loss: 0.1397782665165306, Val Loss: 0.1522921362799879\n",
      "Epoch 14/100, Train Loss: 0.13101908989883354, Val Loss: 0.14779222730866953\n",
      "Epoch 15/100, Train Loss: 0.1232476236924989, Val Loss: 0.1339412005837835\n",
      "Epoch 16/100, Train Loss: 0.11630908345363397, Val Loss: 0.12996525162566763\n",
      "Epoch 17/100, Train Loss: 0.11010033440784725, Val Loss: 0.1201703737147789\n",
      "Epoch 18/100, Train Loss: 0.10452420996865414, Val Loss: 0.11728974912086665\n",
      "Epoch 19/100, Train Loss: 0.09949059642426045, Val Loss: 0.11304701393983078\n",
      "Epoch 20/100, Train Loss: 0.09488799520094143, Val Loss: 0.11572681927534259\n",
      "Epoch 21/100, Train Loss: 0.09065983088880028, Val Loss: 0.10880714357650118\n",
      "Epoch 22/100, Train Loss: 0.08683386238715188, Val Loss: 0.12004993783332675\n",
      "Epoch 23/100, Train Loss: 0.083273060267069, Val Loss: 0.10567809711793187\n",
      "Epoch 24/100, Train Loss: 0.08000446788828607, Val Loss: 0.10336665335738712\n",
      "Epoch 25/100, Train Loss: 0.07698288573097668, Val Loss: 0.09748242108190913\n",
      "Epoch 26/100, Train Loss: 0.07418666489568507, Val Loss: 0.10576087687070249\n",
      "Epoch 27/100, Train Loss: 0.07157707041860033, Val Loss: 0.08362821906074665\n",
      "Epoch 28/100, Train Loss: 0.06918325485147844, Val Loss: 0.07417557304935816\n",
      "Epoch 29/100, Train Loss: 0.0669030113680302, Val Loss: 0.08575485669207542\n",
      "Epoch 30/100, Train Loss: 0.06474711562391755, Val Loss: 0.0763357203754154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rt \u001b[39m=\u001b[39m Critic(load_trained\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rt\u001b[39m.\u001b[39;49mtrain(train_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb Cell 33\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb#X44sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m epoch_validate_loss \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb#X44sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb#X44sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39mfor\u001b[39;00m idx, (feature, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb#X44sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m         feature, label \u001b[39m=\u001b[39m feature\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mlong(), label\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbemidebe/Documents/LSUCourses/Fall2023/CSC7343/homeworks/HW01/PianoGen/Critic.ipynb#X44sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(feature)\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/pianoGen/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rt = Critic(load_trained=False)\n",
    "rt.train(train_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12907"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pianoGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
